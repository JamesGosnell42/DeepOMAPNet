{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94dd74a",
   "metadata": {},
   "source": [
    "# MLP Mapping between GAT Embeddings of RNA and ADT\n",
    "\n",
    "This notebook learns a mapping between GAT embeddings from RNA data and GAT embeddings from ADT data using a Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d23367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization and system check\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set memory management environment variables\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Check system resources\n",
    "print(\"=== System Resources ===\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    gpu_props = torch.cuda.get_device_properties(device)\n",
    "    total_memory = gpu_props.total_memory / (1024**3)  # Convert to GB\n",
    "    \n",
    "    print(f\"GPU: {gpu_props.name}\")\n",
    "    print(f\"Total GPU Memory: {total_memory:.1f} GB\")\n",
    "    print(f\"GPU Compute Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "    \n",
    "    # Clear any cached memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Check current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**3)\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**3)\n",
    "    \n",
    "    print(f\"Currently allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Currently reserved: {reserved:.2f} GB\")\n",
    "    print(f\"Available: {total_memory - reserved:.2f} GB\")\n",
    "    \n",
    "    # Set recommendations based on available memory\n",
    "    if total_memory < 8:\n",
    "        print(\"\\n⚠️  WARNING: Low GPU memory detected!\")\n",
    "        print(\"Recommendations:\")\n",
    "        print(\"- Use CPU fallback if needed\")\n",
    "        print(\"- Reduce batch sizes\")\n",
    "        print(\"- Use graph sparsification\")\n",
    "    elif total_memory < 16:\n",
    "        print(\"\\n💡 Moderate GPU memory - will use optimized settings\")\n",
    "    else:\n",
    "        print(\"\\n✅ Sufficient GPU memory available\")\n",
    "        \n",
    "else:\n",
    "    print(\"CUDA not available - will use CPU\")\n",
    "    print(\"Note: Training will be slower but should work with larger graphs\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set environment variables for better memory management\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "import scanpy as sc\n",
    "import scanpy.external as sce\n",
    "from scipy import sparse\n",
    "\n",
    "from DeepOMAPNet.Preprocess import prepare_train_test_anndata\n",
    "\n",
    "# Set memory management\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4224bfe",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690dfadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "data = prepare_train_test_anndata()\n",
    "trainGene = data[0]  # RNA data\n",
    "trainADT = data[2]   # ADT data\n",
    "\n",
    "print(f\"RNA data shape: {trainGene.shape}\")\n",
    "print(f\"ADT data shape: {trainADT.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bd0f6",
   "metadata": {},
   "source": [
    "## 2. Preprocess RNA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNA preprocessing\n",
    "sc.pp.normalize_total(trainGene, target_sum=1e4)\n",
    "sc.pp.log1p(trainGene)\n",
    "sc.pp.highly_variable_genes(trainGene, n_top_genes=2000, batch_key=\"samples\")\n",
    "trainGene = trainGene[:, trainGene.var.highly_variable].copy()\n",
    "\n",
    "sc.pp.scale(trainGene, max_value=10)\n",
    "sc.tl.pca(trainGene, n_comps=50, svd_solver=\"arpack\")\n",
    "\n",
    "# Build neighbor graph for RNA\n",
    "sc.pp.neighbors(trainGene, n_neighbors=15, n_pcs=50)\n",
    "sc.tl.leiden(trainGene, resolution=1.0)\n",
    "\n",
    "print(f\"RNA data after preprocessing: {trainGene.shape}\")\n",
    "print(f\"Number of RNA clusters: {trainGene.obs['leiden'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882659d1",
   "metadata": {},
   "source": [
    "## 3. Preprocess ADT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADT preprocessing\n",
    "sc.pp.normalize_total(trainADT, target_sum=1e4)\n",
    "sc.pp.log1p(trainADT)\n",
    "sc.pp.scale(trainADT, max_value=10)\n",
    "sc.tl.pca(trainADT, n_comps=50, svd_solver=\"arpack\")\n",
    "\n",
    "# Build neighbor graph for ADT using BBKNN for batch correction\n",
    "sce.pp.bbknn(\n",
    "    trainADT,\n",
    "    batch_key='samples',\n",
    "    n_pcs=50,\n",
    "    neighbors_within_batch=3,\n",
    "    trim=0\n",
    ")\n",
    "\n",
    "sc.tl.leiden(trainADT, resolution=1.0)\n",
    "\n",
    "print(f\"ADT data after preprocessing: {trainADT.shape}\")\n",
    "print(f\"Number of ADT clusters: {trainADT.obs['leiden'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5ac89",
   "metadata": {},
   "source": [
    "## 4. Build PyTorch Geometric Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18511da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_graph(adata, max_edges_per_node=50):\n",
    "    \"\"\"Sparsify the graph by keeping only top k neighbors per node\"\"\"\n",
    "    \n",
    "    A = adata.obsp[\"connectivities\"].tocsr()\n",
    "    n_nodes = A.shape[0]\n",
    "    \n",
    "    # Check if sparsification is needed\n",
    "    avg_degree = A.nnz / n_nodes\n",
    "    if avg_degree <= max_edges_per_node:\n",
    "        print(f\"Graph already sparse enough (avg degree: {avg_degree:.1f})\")\n",
    "        return adata\n",
    "    \n",
    "    print(f\"Sparsifying graph from avg degree {avg_degree:.1f} to max {max_edges_per_node}\")\n",
    "    \n",
    "    # Create new sparse matrix\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data_values = []\n",
    "    \n",
    "    for i in range(n_nodes):\n",
    "        # Get neighbors and their weights for node i\n",
    "        start_idx = A.indptr[i]\n",
    "        end_idx = A.indptr[i + 1]\n",
    "        neighbors = A.indices[start_idx:end_idx]\n",
    "        weights = A.data[start_idx:end_idx]\n",
    "        \n",
    "        # Keep only top k neighbors\n",
    "        if len(neighbors) > max_edges_per_node:\n",
    "            top_k_indices = np.argpartition(weights, -max_edges_per_node)[-max_edges_per_node:]\n",
    "            neighbors = neighbors[top_k_indices]\n",
    "            weights = weights[top_k_indices]\n",
    "        \n",
    "        # Add edges\n",
    "        row_indices.extend([i] * len(neighbors))\n",
    "        col_indices.extend(neighbors)\n",
    "        data_values.extend(weights)\n",
    "    \n",
    "    # Create new adjacency matrix\n",
    "    A_sparse = sparse.csr_matrix(\n",
    "        (data_values, (row_indices, col_indices)), \n",
    "        shape=(n_nodes, n_nodes)\n",
    "    )\n",
    "    \n",
    "    # Make symmetric\n",
    "    A_sparse = (A_sparse + A_sparse.T) / 2\n",
    "    \n",
    "    # Update the AnnData object\n",
    "    adata.obsp[\"connectivities\"] = A_sparse\n",
    "    \n",
    "    new_avg_degree = A_sparse.nnz / n_nodes\n",
    "    print(f\"New average degree: {new_avg_degree:.1f}\")\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def build_pyg_data(adata, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=50):\n",
    "    \"\"\"Build PyTorch Geometric Data object from AnnData\"\"\"\n",
    "    \n",
    "    # Sparsify if needed\n",
    "    if sparsify_large_graphs:\n",
    "        A = adata.obsp[\"connectivities\"]\n",
    "        avg_degree = A.nnz / A.shape[0]\n",
    "        if avg_degree > max_edges_per_node:\n",
    "            print(f\"Large graph detected (avg degree: {avg_degree:.1f}), applying sparsification...\")\n",
    "            adata = sparsify_graph(adata, max_edges_per_node)\n",
    "    \n",
    "    # Features\n",
    "    X = adata.obsm[\"X_pca\"] if use_pca else adata.X.toarray()\n",
    "    \n",
    "    # Labels (leiden clusters)\n",
    "    y = adata.obs[\"leiden\"].astype(int).to_numpy()\n",
    "    \n",
    "    # Edge index from connectivities\n",
    "    A = adata.obsp[\"connectivities\"].tocsr()\n",
    "    A_triu = sparse.triu(A, k=1)\n",
    "    row, col = A_triu.nonzero()\n",
    "    edge_index = torch.tensor(np.vstack([row, col]), dtype=torch.long)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(\n",
    "        x=torch.tensor(X, dtype=torch.float32),\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor(y, dtype=torch.long),\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Build data objects with memory optimization\n",
    "print(\"Building PyG data objects...\")\n",
    "\n",
    "# Check available GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
    "    print(f\"Available GPU memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Estimate memory requirements\n",
    "    rna_edges = trainGene.obsp[\"connectivities\"].nnz\n",
    "    adt_edges = trainADT.obsp[\"connectivities\"].nnz\n",
    "    \n",
    "    print(f\"RNA graph edges: {rna_edges:,}\")\n",
    "    print(f\"ADT graph edges: {adt_edges:,}\")\n",
    "    \n",
    "    # Set sparsification based on graph size\n",
    "    max_edges_rna = 100 if rna_edges > 5000000 else 200\n",
    "    max_edges_adt = 50 if adt_edges > 10000000 else 100\n",
    "    \n",
    "    print(f\"Using max edges per node - RNA: {max_edges_rna}, ADT: {max_edges_adt}\")\n",
    "else:\n",
    "    print(\"Using CPU - no memory constraints\")\n",
    "    max_edges_rna = 200\n",
    "    max_edges_adt = 100\n",
    "\n",
    "# Build data objects\n",
    "rna_data = build_pyg_data(trainGene, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=max_edges_rna)\n",
    "adt_data = build_pyg_data(trainADT, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=max_edges_adt)\n",
    "\n",
    "print(f\"RNA PyG data - Nodes: {rna_data.num_nodes}, Edges: {rna_data.num_edges}, Features: {rna_data.num_node_features}\")\n",
    "print(f\"ADT PyG data - Nodes: {adt_data.num_nodes}, Edges: {adt_data.num_edges}, Features: {adt_data.num_node_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c8c4c8",
   "metadata": {},
   "source": [
    "## 5. Define GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfef761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGAT(torch.nn.Module):\n",
    "    \"\"\"Simplified GAT for memory-constrained scenarios\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Single GAT layer for memory efficiency\n",
    "        self.conv1 = GATConv(in_channels, out_channels, heads=heads, dropout=dropout, concat=False)\n",
    "        \n",
    "    def forward(self, x, edge_index, return_embeddings=False):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return x\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get embeddings for mapping\"\"\"\n",
    "        return self.forward(x, edge_index, return_embeddings=True)\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=8, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index, return_embeddings=False):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return x  # Return embeddings before final layer\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get intermediate embeddings for mapping\"\"\"\n",
    "        return self.forward(x, edge_index, return_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822955f3",
   "metadata": {},
   "source": [
    "## 6. Train GAT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb69fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat_model(data, model_name=\"GAT\", epochs=200, use_cpu_fallback=False):\n",
    "    \"\"\"Train a GAT model and return the trained model\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and not use_cpu_fallback else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Check memory requirements and adjust accordingly\n",
    "    num_edges = data.num_edges\n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    print(f\"Graph stats - Nodes: {num_nodes}, Edges: {num_edges}\")\n",
    "    \n",
    "    # Memory optimization: reduce model size if too many edges\n",
    "    use_simple_model = False\n",
    "    if num_edges > 2000000:  # If more than 2M edges\n",
    "        print(\"Very large graph detected, using simplified GAT architecture...\")\n",
    "        hidden_dim = 32\n",
    "        heads = 4\n",
    "        use_simple_model = True\n",
    "    elif num_edges > 1000000:  # If more than 1M edges\n",
    "        print(\"Large graph detected, reducing model complexity...\")\n",
    "        hidden_dim = 32\n",
    "        heads = 4\n",
    "    else:\n",
    "        hidden_dim = 64\n",
    "        heads = 8\n",
    "    \n",
    "    # Create train/val/test masks\n",
    "    N = data.num_nodes\n",
    "    y_np = data.y.cpu().numpy()\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    \n",
    "    # Split 80/10/10\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "    train_idx, temp_idx = next(sss1.split(np.zeros(N), y_np))\n",
    "    \n",
    "    y_temp = y_np[temp_idx]\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.5, random_state=43)\n",
    "    val_rel, test_rel = next(sss2.split(np.zeros(len(temp_idx)), y_temp))\n",
    "    val_idx = temp_idx[val_rel]\n",
    "    test_idx = temp_idx[test_rel]\n",
    "    \n",
    "    train_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "    # Initialize model\n",
    "    in_dim = data.x.size(1)\n",
    "    n_class = int(data.y.max().item() + 1)\n",
    "    \n",
    "    if use_simple_model:\n",
    "        model = SimpleGAT(in_dim, hidden_dim, n_class, heads=heads).to(device)\n",
    "        print(f\"Using SimpleGAT: {in_dim} -> {n_class} (hidden: {hidden_dim}, heads: {heads})\")\n",
    "    else:\n",
    "        model = GAT(in_dim, hidden_dim, n_class, heads=heads).to(device)\n",
    "        print(f\"Using GAT: {in_dim} -> {hidden_dim} -> {n_class} (heads: {heads})\")\n",
    "    \n",
    "    # Move data to device with memory management\n",
    "    cpu_fallback_triggered = False\n",
    "    try:\n",
    "        data = data.to(device)\n",
    "        print(f\"Successfully moved data to {device}\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"GPU memory insufficient, falling back to CPU...\")\n",
    "            device = torch.device('cpu')\n",
    "            model = model.cpu()\n",
    "            data = data.cpu()\n",
    "            cpu_fallback_triggered = True\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Initialize optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train():\n",
    "        nonlocal model, data, optimizer, device, cpu_fallback_triggered\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()  # Clear cache before forward pass\n",
    "            \n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()  # Clear cache after backward pass\n",
    "                \n",
    "            return loss\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower() and not cpu_fallback_triggered:\n",
    "                print(f\"GPU OOM during training, switching to CPU...\")\n",
    "                # Move everything to CPU\n",
    "                device = torch.device('cpu')\n",
    "                model = model.cpu()\n",
    "                data = data.cpu()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "                cpu_fallback_triggered = True\n",
    "                \n",
    "                # Retry the forward pass on CPU\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data.x, data.edge_index)\n",
    "                loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                return loss\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    def test(mask):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            out = model(data.x, data.edge_index)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred[mask] == data.y[mask]\n",
    "            acc = int(correct.sum()) / int(mask.sum())\n",
    "            return acc\n",
    "    \n",
    "    print(f\"Training {model_name} model...\")\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train()\n",
    "        \n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            val_acc = test(data.val_mask)\n",
    "            test_acc = test(data.test_mask)\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    final_test_acc = test(data.test_mask)\n",
    "    print(f\"Final {model_name} test accuracy: {final_test_acc:.4f}\")\n",
    "    \n",
    "    return model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a9e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAT models for both RNA and ADT with memory management\n",
    "print(\"=== Training RNA GAT ===\")\n",
    "try:\n",
    "    rna_gat_model, rna_data_with_masks = train_gat_model(rna_data, \"RNA GAT\", epochs=200)\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU memory insufficient for RNA GAT, trying CPU...\")\n",
    "        rna_gat_model, rna_data_with_masks = train_gat_model(rna_data, \"RNA GAT\", epochs=200, use_cpu_fallback=True)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(\"\\n=== Training ADT GAT ===\")\n",
    "try:\n",
    "    adt_gat_model, adt_data_with_masks = train_gat_model(adt_data, \"ADT GAT\", epochs=200)\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU memory insufficient for ADT GAT, trying CPU...\")\n",
    "        adt_gat_model, adt_data_with_masks = train_gat_model(adt_data, \"ADT GAT\", epochs=200, use_cpu_fallback=True)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(\"\\n=== GAT Training Complete ===\")\n",
    "print(f\"RNA GAT model trained successfully\")\n",
    "print(f\"ADT GAT model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfc8fd",
   "metadata": {},
   "source": [
    "## 7. Extract GAT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, data):\n",
    "    \"\"\"Extract embeddings from trained GAT model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure model and data are on the same device\n",
    "    device = next(model.parameters()).device\n",
    "    if data.x.device != device:\n",
    "        print(f\"Moving data from {data.x.device} to {device}\")\n",
    "        data = data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Clear cache if using GPU\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        embeddings = model.get_embeddings(data.x, data.edge_index)\n",
    "        \n",
    "        # Move to CPU for further processing\n",
    "        embeddings = embeddings.cpu()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "# Clear any existing cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Extract embeddings\n",
    "print(\"Extracting embeddings...\")\n",
    "rna_embeddings = extract_embeddings(rna_gat_model, rna_data_with_masks)\n",
    "adt_embeddings = extract_embeddings(adt_gat_model, adt_data_with_masks)\n",
    "\n",
    "print(f\"RNA embeddings shape: {rna_embeddings.shape}\")\n",
    "print(f\"ADT embeddings shape: {adt_embeddings.shape}\")\n",
    "\n",
    "# Ensure both embeddings have the same number of cells\n",
    "assert rna_embeddings.shape[0] == adt_embeddings.shape[0], \"Number of cells must match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e123e",
   "metadata": {},
   "source": [
    "## 8. Define MLP Mapping Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMapping(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super(MLPMapping, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "# Initialize MLP mapping model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_dim = rna_embeddings.shape[1]\n",
    "output_dim = adt_embeddings.shape[1]\n",
    "\n",
    "mlp_model = MLPMapping(input_dim, output_dim, hidden_dims=[256, 128, 64]).to(device)\n",
    "\n",
    "print(f\"MLP Model: {input_dim} -> {output_dim}\")\n",
    "print(mlp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740489e",
   "metadata": {},
   "source": [
    "## 9. Prepare Training Data for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e65bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to CPU and numpy\n",
    "rna_emb_np = rna_embeddings.cpu().numpy()\n",
    "adt_emb_np = adt_embeddings.cpu().numpy()\n",
    "\n",
    "# Split data for MLP training (use same train/val/test split as GAT)\n",
    "train_mask_np = rna_data_with_masks.train_mask.cpu().numpy()\n",
    "val_mask_np = rna_data_with_masks.val_mask.cpu().numpy()\n",
    "test_mask_np = rna_data_with_masks.test_mask.cpu().numpy()\n",
    "\n",
    "# Prepare training data\n",
    "X_train = torch.tensor(rna_emb_np[train_mask_np], dtype=torch.float32)\n",
    "y_train = torch.tensor(adt_emb_np[train_mask_np], dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(rna_emb_np[val_mask_np], dtype=torch.float32)\n",
    "y_val = torch.tensor(adt_emb_np[val_mask_np], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(rna_emb_np[test_mask_np], dtype=torch.float32)\n",
    "y_test = torch.tensor(adt_emb_np[test_mask_np], dtype=torch.float32)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5f66c",
   "metadata": {},
   "source": [
    "## 10. Train MLP Mapping Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience = 50\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Training MLP mapping model...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    mlp_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    mlp_model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = mlp_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = mlp_model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "mlp_model.load_state_dict(best_model_state)\n",
    "print(f'Best validation loss: {best_val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad3f0b",
   "metadata": {},
   "source": [
    "## 11. Evaluate MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "mlp_model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = mlp_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        ground_truth.append(batch_y.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "predictions = np.vstack(predictions)\n",
    "ground_truth = np.vstack(ground_truth)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(ground_truth, predictions)\n",
    "r2 = r2_score(ground_truth, predictions)\n",
    "\n",
    "# Calculate correlation per dimension\n",
    "pearson_corrs = []\n",
    "spearman_corrs = []\n",
    "\n",
    "for i in range(ground_truth.shape[1]):\n",
    "    pearson_r, _ = pearsonr(ground_truth[:, i], predictions[:, i])\n",
    "    spearman_r, _ = spearmanr(ground_truth[:, i], predictions[:, i])\n",
    "    pearson_corrs.append(pearson_r)\n",
    "    spearman_corrs.append(spearman_r)\n",
    "\n",
    "mean_pearson = np.mean(pearson_corrs)\n",
    "mean_spearman = np.mean(spearman_corrs)\n",
    "\n",
    "print(f\"\\n=== MLP Mapping Results ===\")\n",
    "print(f\"Test Loss (MSE): {test_loss:.6f}\")\n",
    "print(f\"MSE: {mse:.6f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Mean Pearson Correlation: {mean_pearson:.4f}\")\n",
    "print(f\"Mean Spearman Correlation: {mean_spearman:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755ca64",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pearson_corrs, bins=20, alpha=0.7, label='Pearson')\n",
    "plt.hist(spearman_corrs, bins=20, alpha=0.7, label='Spearman')\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Per-dimension Correlation Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for first few dimensions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, ground_truth.shape[1])):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(ground_truth[:, i], predictions[:, i], alpha=0.6, s=1)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(ground_truth[:, i].min(), predictions[:, i].min())\n",
    "    max_val = max(ground_truth[:, i].max(), predictions[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel(f'True ADT Embedding Dim {i+1}')\n",
    "    ax.set_ylabel(f'Predicted ADT Embedding Dim {i+1}')\n",
    "    ax.set_title(f'Dim {i+1}: r={pearson_corrs[i]:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa477ec4",
   "metadata": {},
   "source": [
    "## 13. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "torch.save({\n",
    "    'rna_gat_state_dict': rna_gat_model.state_dict(),\n",
    "    'adt_gat_state_dict': adt_gat_model.state_dict(),\n",
    "    'mlp_mapping_state_dict': mlp_model.state_dict(),\n",
    "    'rna_input_dim': input_dim,\n",
    "    'adt_output_dim': output_dim,\n",
    "    'test_results': {\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'mean_pearson': mean_pearson,\n",
    "        'mean_spearman': mean_spearman,\n",
    "        'pearson_corrs': pearson_corrs,\n",
    "        'spearman_corrs': spearman_corrs\n",
    "    }\n",
    "}, 'rna_adt_mapping_models.pth')\n",
    "\n",
    "print(\"Models and results saved to 'rna_adt_mapping_models.pth'\")\n",
    "\n",
    "# Save predictions for further analysis\n",
    "np.savez('mapping_predictions.npz', \n",
    "         predictions=predictions, \n",
    "         ground_truth=ground_truth,\n",
    "         pearson_corrs=pearson_corrs,\n",
    "         spearman_corrs=spearman_corrs)\n",
    "\n",
    "print(\"Predictions saved to 'mapping_predictions.npz'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8040f9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a pipeline to learn mappings between GAT embeddings of RNA and ADT data:\n",
    "\n",
    "1. **Data Preprocessing**: Both RNA and ADT data are normalized, scaled, and processed to create neighbor graphs\n",
    "2. **GAT Training**: Separate GAT models are trained on RNA and ADT data for node classification\n",
    "3. **Embedding Extraction**: Intermediate embeddings are extracted from the trained GAT models\n",
    "4. **MLP Mapping**: A multi-layer perceptron learns to map RNA embeddings to ADT embeddings\n",
    "5. **Evaluation**: The mapping quality is assessed using MSE, R², and correlation metrics\n",
    "\n",
    "The trained models can be used to predict ADT embeddings from RNA data, enabling cross-modal analysis and integration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
